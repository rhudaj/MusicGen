{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13eaeb9",
   "metadata": {},
   "source": [
    "## TODOS\n",
    "\n",
    "1. How to handle multiple notes at same time-step? (e.g. chords, drumming)\n",
    "2. How to handle note loudness?\n",
    "3. How to handle silence?  \n",
    "4. How to seed different predictions? \n",
    "5. Select `key` of song\n",
    "6. How to handle sections (e.g. verse, chorus, etc.)?\n",
    "\t- Need to do `pattern analysis`\n",
    "7. Should loss depend on what note we're guessing (e.g. we care more about predicting later notes than earlier ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56406e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform; platform.mac_ver()\n",
    "# Should be 12.3 or greater, and 'arm64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a4e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "import pypianoroll as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95d2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal Imports\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "from src.util.types import Song, PianoState, NoteSample, PianoStateSamples\n",
    "from src.util.globals import resolution, beats_per_bar, num_pitches, DEVICE\n",
    "from src.util.convert import (\n",
    "\toutput_piannoroll_to_midi\n",
    ")\n",
    "from src.util.plot import plot_pianoroll, plot_piano_states, plot_note_sample_probs, plot_track\n",
    "from src.models import MusicRNN, MusicRNN_Batched, MusicLSTM\n",
    "from src.models.train import train_batched\n",
    "from src.models.infer import sample_notes\n",
    "\n",
    "from src.dataset.dataset import InstrumentDataset, get_dataloader\n",
    "from src.dataset.load import (\n",
    "    load_multi_track,\n",
    "    get_track_by_instrument,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Plot a specific file/track\n",
    "if False:\n",
    "\tdesired_instrument = 'Guitar'\n",
    "\tmulti_track = load_multi_track(f'A/A/A/TRAAAGR128F425B14B/b97c529ab9ef783a849b896816001748.npz')\n",
    "\tpr.plot_multitrack(multi_track, axs=None, mode='blended')\n",
    "\n",
    "\ttrack = get_track_by_instrument(multi_track, desired_instrument)\n",
    "\n",
    "\tif track:\n",
    "\t\tplot_track(track, desired_instrument, True, 4)\n",
    "\telse:\n",
    "\t\tprint('No track found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa660f",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2851e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\tbasic_model = MusicRNN(\n",
    "\t\thidden_size=128,\n",
    "\t\tnum_pitches=129,  # 0-128 notes (including silence at 0)\n",
    "\t\tnum_layers=2,\n",
    "\t\tdropout=0.1\n",
    "\t).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52165e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\tbatched_model = MusicRNN_Batched(\n",
    "\t\thidden_size=128,\n",
    "\t\tnum_pitches=129,  # 0-128 notes (including silence at 0)\n",
    "\t\tnum_layers=2,\n",
    "\t\tdropout=0.1\n",
    "\t)\n",
    "\n",
    "\tbatched_model = batched_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35584c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\tlstm_model = MusicLSTM(\n",
    "\t\thidden_size=256,\n",
    "\t\tnum_pitches=num_pitches+1,  # 0-128 notes (including silence at 0)\n",
    "\t\tnum_layers=2,\n",
    "\t\tdropout=0.1\n",
    "\t).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f8513",
   "metadata": {},
   "source": [
    "## Testing (toy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eca953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some simple test sequences\n",
    "test_seq_1 = torch.Tensor([\n",
    "\t[1, 10],\n",
    "\t[2, 10],\n",
    " \t[3, 10],\n",
    "  \t[4, 10],\n",
    "   \t[5, 10],\n",
    "    [4, 10],\n",
    "    [3, 10],\n",
    "    [2, 10],\n",
    "    [1, 10],\n",
    "\t[2, 10],\n",
    " \t[3, 10],\n",
    "  \t[4, 10],\n",
    "   \t[5, 10],\n",
    "    [4, 10],\n",
    "    [3, 10],\n",
    "    [2, 10],\n",
    "    [1, 10],\n",
    "]).float()\n",
    "\n",
    "test_seq_2 = torch.Tensor([\n",
    "\t[1, 10],\n",
    "\t[2, 10],\n",
    " \t[3, 10],\n",
    "  \t[4, 10],\n",
    "   \t[5, 10]\n",
    "]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeedd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Overfit on 1 sequence\n",
    "if False:\n",
    "\tmodel = lstm_model\n",
    "\n",
    "\tseq = test_seq_1\n",
    "\tstart_notes = seq[0]\n",
    "\tmax_len = 100\n",
    "\n",
    "\tif True:\n",
    "\t\ttrain_batched(model, [seq], num_epochs=5000, lr=0.0001)\n",
    "\n",
    "\t\t# Test sampling a sequence\n",
    "\t\tpiano_state_samples = sample_notes(model, start_notes, max_len)\n",
    "\n",
    "\t\tplot_piano_states(seq, None, 'Real Sequence')\n",
    "\t\tplot_piano_states(piano_state_samples.piano_states, None, 'Generated Sequence')\n",
    "\t\tplot_note_sample_probs(piano_state_samples.note_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa784db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Packed Model\n",
    "seqs = [test_seq_1, test_seq_2]\n",
    "start_notes = seqs[0][0]\n",
    "max_len = 100\n",
    "\n",
    "if False:\n",
    "\ttrain_batched(\n",
    "     \tbatched_model,\n",
    "      \tseqs,\n",
    "\t\tbatch_size=2,\n",
    "       \tnum_epochs=1000,\n",
    "        lr=0.0001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Test sampling a sequence\n",
    "\tpiano_state_samples = sample_notes(\n",
    "     \tbatched_model,\n",
    "      \tstart_notes,\n",
    "       \tmax_len,\n",
    "\t\ttemperature=0.3\n",
    "    )\n",
    "\n",
    "\t# plot_piano_states(seq, None, 'Real Sequence')\n",
    "\tplot_piano_states(piano_state_samples.piano_states, None, 'Generated Sequence')\n",
    "\tplot_note_sample_probs(piano_state_samples.note_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e084ebe",
   "metadata": {},
   "source": [
    "## Testing (Real Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "215e47f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21425 total files\n",
      "Got 20 total sequences for instrument \"Bass\"\n"
     ]
    }
   ],
   "source": [
    "dataset = InstrumentDataset(\n",
    "\tinstrument='Bass',\n",
    "\tmax_samples=20,\n",
    ")\n",
    "trainloader = get_dataloader(dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89dc53f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicLSTM(\n",
      "  (rnn): LSTM(2, 256, num_layers=2, bias=False, batch_first=True, dropout=0.1)\n",
      "  (note_head): Linear(in_features=256, out_features=130, bias=True)\n",
      "  (duration_head): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "# Parameters: 822147\n"
     ]
    }
   ],
   "source": [
    "# Choose your model\n",
    "model = lstm_model\n",
    "print(model)\n",
    "print(f\"# Parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880d3c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on data set with n = 4\n",
      "Epoch 0/200, Loss = 54710613.7500\n",
      "Epoch 1/200, Loss = 53714266.0000\n",
      "Epoch 2/200, Loss = 53002698.5000\n",
      "Epoch 3/200, Loss = 52690584.2500\n",
      "Epoch 4/200, Loss = 52543467.2500\n",
      "Epoch 5/200, Loss = 52461307.5000\n",
      "Epoch 6/200, Loss = 52398194.0000\n",
      "Epoch 7/200, Loss = 52351616.2500\n",
      "Epoch 8/200, Loss = 52314846.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m \t\u001b[43mtrain_batched\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m\t\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/ML/MusicGen/src/models/train.py:153\u001b[0m, in \u001b[0;36mtrain_batched\u001b[0;34m(model, dataloader, num_epochs, lr)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (packed_batch, batch_size) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    151\u001b[0m     packed_batch \u001b[38;5;241m=\u001b[39m packed_batch\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> 153\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_fully_packed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpacked_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnote_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    163\u001b[0m epoch_losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "File \u001b[0;32m~/Desktop/projects/ML/MusicGen/src/models/train.py:101\u001b[0m, in \u001b[0;36mtrain_step_fully_packed\u001b[0;34m(model, packed_input_batch, batch_size, note_criterion, duration_criterion, optimizer, clip_grads, max_norm)\u001b[0m\n\u001b[1;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m packed_note_logits, packed_duration_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_input_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Create targets from inputs (shift by one timestep)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# For packed sequences, we need to shift the data portion\u001b[39;00m\n\u001b[1;32m    105\u001b[0m input_data \u001b[38;5;241m=\u001b[39m packed_input_batch\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Remove first timestep\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/ML/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/ML/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/projects/ML/MusicGen/src/models/lstm/model.py:53\u001b[0m, in \u001b[0;36mMusicLSTM.forward\u001b[0;34m(self, packed_input)\u001b[0m\n\u001b[1;32m     45\u001b[0m \t\u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, packed_input: PackedSequence):\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m \t\t\u001b[38;5;66;03m# Initialize internal states\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \t\t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_hidden(\n\u001b[1;32m     49\u001b[0m \t\t\t\u001b[38;5;66;03m# infer the batch size\u001b[39;00m\n\u001b[1;32m     50\u001b[0m       \t\tbatch_size\u001b[38;5;241m=\u001b[39mpacked_input\u001b[38;5;241m.\u001b[39mbatch_sizes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     51\u001b[0m     \t)\n\u001b[0;32m---> 53\u001b[0m \t\toutput_packed, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \t\t\u001b[38;5;66;03m# `output_packed.data`` shape: (total_elements, D * hidden_size)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m   \t\t\u001b[38;5;66;03m# NOTE: `total_elements` = sum of all sequence lengths in the batch\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m \t\t\u001b[38;5;66;03m# Apply output heads directly to packed data\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \t\tnote_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnote_head(output_packed\u001b[38;5;241m.\u001b[39mdata)  \t\t\u001b[38;5;66;03m# Shape: (total_elements, num_pitches)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/ML/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/ML/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/projects/ML/venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:1136\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1126\u001b[0m         hx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[1;32m   1134\u001b[0m     )\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1148\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if True:\n",
    "\ttrain_batched(\n",
    "\t\tmodel,\n",
    "\t\ttrainloader,\n",
    "\t\tnum_epochs=200,\n",
    "\t\tlr=0.001\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abe5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = dataset[0]\n",
    "if True:\n",
    "\tpredictions = sample_notes(\n",
    "\t\tmodel,\n",
    "\t\tstart_event=torch.Tensor(seq[0]),\n",
    "\t\tlength=beats_per_bar*1,\n",
    "\t\ttemperature=0.3\n",
    "\t)\n",
    "\n",
    "\tplot_note_sample_probs(predictions.note_samples)\n",
    "\toutput_piannoroll_to_midi(\n",
    "     \tpredictions.piano_states,\n",
    "\t\tinstrument='Guitar',\n",
    "     \tname='generated_guitar2'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
