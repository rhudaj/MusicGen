{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13eaeb9",
   "metadata": {},
   "source": [
    "## TODOS\n",
    "\n",
    "1. How to handle multiple notes at same time-step? (e.g. chords, drumming)\n",
    "2. How to handle note loudness?\n",
    "3. How to handle silence?  \n",
    "4. How to seed different predictions? \n",
    "5. Select `key` of song\n",
    "6. How to handle sections (e.g. verse, chorus, etc.)?\n",
    "\t- Need to do `pattern analysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56406e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('14.5', ('', '', ''), 'arm64')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform; platform.mac_ver()\n",
    "# Should be 12.3 or greater, and 'arm64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a4e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "import pypianoroll as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95d2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal Imports\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('/src'))\n",
    "\n",
    "from src.util.types import Song, PianoState, NoteSample, PianoStateSamples\n",
    "from src.util.globals import resolution, beats_per_bar, num_pitches, DEVICE\n",
    "from src.util.convert import (\n",
    "\toutput_piannoroll_to_midi\n",
    ")\n",
    "from src.util.plot import plot_pianoroll, plot_piano_states, plot_note_sample_probs, plot_track\n",
    "from src.models import MusicRNN, MusicRNN_Batched, MusicLSTM\n",
    "from src.models.train import train, train_batched\n",
    "from src.models.infer import sample_notes\n",
    "\n",
    "from src.dataset.dataset import InstrumentDataset, get_dataloader\n",
    "from src.dataset.load import (\n",
    "    get_songs,\n",
    "    load_multi_track,\n",
    "    get_track_by_instrument,\n",
    "    get_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e3f6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Plot a specific file/track\n",
    "if False:\n",
    "\tdesired_instrument = 'Guitar'\n",
    "\tmulti_track = load_multi_track(f'A/A/A/TRAAAGR128F425B14B/b97c529ab9ef783a849b896816001748.npz')\n",
    "\tpr.plot_multitrack(multi_track, axs=None, mode='blended')\n",
    "\n",
    "\ttrack = get_track_by_instrument(multi_track, desired_instrument)\n",
    "\n",
    "\tif track:\n",
    "\t\tplot_track(track, desired_instrument, True, 4)\n",
    "\telse:\n",
    "\t\tprint('No track found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa660f",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2851e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\tbasic_model = MusicRNN(\n",
    "\t\thidden_size=128,\n",
    "\t\tnum_pitches=129,  # 0-128 notes (including silence at 0)\n",
    "\t\tnum_layers=2,\n",
    "\t\tdropout=0.1\n",
    "\t).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52165e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\tbatched_model = MusicRNN_Batched(\n",
    "\t\thidden_size=128,\n",
    "\t\tnum_pitches=129,  # 0-128 notes (including silence at 0)\n",
    "\t\tnum_layers=2,\n",
    "\t\tdropout=0.1\n",
    "\t)\n",
    "\n",
    "\tbatched_model = batched_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e35584c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\tlstm_model = MusicLSTM(\n",
    "\t\thidden_size=256,\n",
    "\t\tnum_pitches=num_pitches+1,  # 0-128 notes (including silence at 0)\n",
    "\t\tnum_layers=2,\n",
    "\t\tdropout=0.1\n",
    "\t).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f8513",
   "metadata": {},
   "source": [
    "## Testing (toy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13eca953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some simple test sequences\n",
    "test_seq_1 = torch.Tensor([\n",
    "\t[1, 10],\n",
    "\t[2, 10],\n",
    " \t[3, 10],\n",
    "  \t[4, 10],\n",
    "   \t[5, 10],\n",
    "    [4, 10],\n",
    "    [3, 10],\n",
    "    [2, 10],\n",
    "    [1, 10],\n",
    "\t[2, 10],\n",
    " \t[3, 10],\n",
    "  \t[4, 10],\n",
    "   \t[5, 10],\n",
    "    [4, 10],\n",
    "    [3, 10],\n",
    "    [2, 10],\n",
    "    [1, 10],\n",
    "]).float()\n",
    "\n",
    "test_seq_2 = torch.Tensor([\n",
    "\t[1, 10],\n",
    "\t[2, 10],\n",
    " \t[3, 10],\n",
    "  \t[4, 10],\n",
    "   \t[5, 10]\n",
    "]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aeedd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Overfit on 1 sequence\n",
    "if False:\n",
    "\tmodel = lstm_model\n",
    "\n",
    "\tseq = test_seq_1\n",
    "\tstart_notes = seq[0]\n",
    "\tmax_len = 100\n",
    "\n",
    "\tif True:\n",
    "\t\ttrain_batched(model, [seq], num_epochs=5000, lr=0.0001)\n",
    "\n",
    "\t\t# Test sampling a sequence\n",
    "\t\tpiano_state_samples = sample_notes(model, start_notes, max_len)\n",
    "\n",
    "\t\tplot_piano_states(seq, None, 'Real Sequence')\n",
    "\t\tplot_piano_states(piano_state_samples.piano_states, None, 'Generated Sequence')\n",
    "\t\tplot_note_sample_probs(piano_state_samples.note_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa784db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Packed Model\n",
    "seqs = [test_seq_1, test_seq_2]\n",
    "start_notes = seqs[0][0]\n",
    "max_len = 100\n",
    "\n",
    "if False:\n",
    "\ttrain_batched(\n",
    "     \tbatched_model,\n",
    "      \tseqs,\n",
    "\t\tbatch_size=2,\n",
    "       \tnum_epochs=1000,\n",
    "        lr=0.0001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f186ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Test sampling a sequence\n",
    "\tpiano_state_samples = sample_notes(\n",
    "     \tbatched_model,\n",
    "      \tstart_notes,\n",
    "       \tmax_len,\n",
    "\t\ttemperature=0.3\n",
    "    )\n",
    "\n",
    "\t# plot_piano_states(seq, None, 'Real Sequence')\n",
    "\tplot_piano_states(piano_state_samples.piano_states, None, 'Generated Sequence')\n",
    "\tplot_note_sample_probs(piano_state_samples.note_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e084ebe",
   "metadata": {},
   "source": [
    "## Testing (Real Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "215e47f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21425 total files\n",
      "Got 20 total sequences for instrument \"Bass\"\n"
     ]
    }
   ],
   "source": [
    "dataset = InstrumentDataset(\n",
    "\tinstrument='Bass',\n",
    "\tmax_samples=20,\n",
    ")\n",
    "trainloader = get_dataloader(dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89dc53f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicLSTM(\n",
      "  (rnn): LSTM(2, 256, num_layers=2, bias=False, batch_first=True, dropout=0.1)\n",
      "  (note_head): Linear(in_features=256, out_features=130, bias=True)\n",
      "  (duration_head): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "# Parameters: 822147\n"
     ]
    }
   ],
   "source": [
    "# Choose your model\n",
    "model = lstm_model\n",
    "print(model)\n",
    "print(f\"# Parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d3c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on data set with n = 4\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "\ttrain_batched(\n",
    "\t\tmodel,\n",
    "\t\ttrainloader,\n",
    "\t\tnum_epochs=200,\n",
    "\t\tlr=0.001\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abe5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = dataset[0]\n",
    "if True:\n",
    "\tpredictions = sample_notes(\n",
    "\t\tmodel,\n",
    "\t\tstart_event=torch.Tensor(seq[0]),\n",
    "\t\tlength=beats_per_bar*1,\n",
    "\t\ttemperature=0.3\n",
    "\t)\n",
    "\n",
    "\tplot_note_sample_probs(predictions.note_samples)\n",
    "\toutput_piannoroll_to_midi(\n",
    "     \tpredictions.piano_states,\n",
    "\t\tinstrument='Guitar',\n",
    "     \tname='generated_guitar2'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
