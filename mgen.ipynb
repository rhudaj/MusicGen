{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13eaeb9",
   "metadata": {},
   "source": [
    "## TODOS\n",
    "\n",
    "1. How to handle multiple notes at same time-step? (e.g. chords, drumming)\n",
    "2. How to handle note loudness?\n",
    "3. How to handle silence?  \n",
    "4. How to seed different predictions? \n",
    "5. Select `key` of song\n",
    "6. How to handle sections (e.g. verse, chorus, etc.)?\n",
    "\t- Need to do `pattern analysis`\n",
    "7. Should loss depend on what note we're guessing (e.g. we care more about predicting later notes than earlier ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56406e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('14.5', ('', '', ''), 'arm64')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform; platform.mac_ver()\n",
    "# Should be 12.3 or greater, and 'arm64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a4e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "import pypianoroll as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal Imports\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('/src'))\n",
    "\n",
    "from util.types import Song, PianoState, NoteSample, PianoStateSamples\n",
    "from util.globals import resolution, beats_per_bar, num_pitches, DEVICE\n",
    "from util.convert import (\n",
    "\toutput_piannoroll_to_midi\n",
    ")\n",
    "from util.plot import plot_pianoroll, plot_piano_states, plot_note_sample_probs, plot_track\n",
    "from models import MusicRNN, MusicRNN_Batched, MusicLSTM\n",
    "from models.train import train, train_batched\n",
    "from models.infer import sample_notes\n",
    "\n",
    "from dataset.dataset import InstrumentDataset, get_dataloader\n",
    "from dataset.load import (\n",
    "    get_songs,\n",
    "    load_multi_track,\n",
    "    get_track_by_instrument,\n",
    "    get_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e3f6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Plot a specific file/track\n",
    "if False:\n",
    "\tdesired_instrument = 'Guitar'\n",
    "\tmulti_track = load_multi_track(f'A/A/A/TRAAAGR128F425B14B/b97c529ab9ef783a849b896816001748.npz')\n",
    "\tpr.plot_multitrack(multi_track, axs=None, mode='blended')\n",
    "\n",
    "\ttrack = get_track_by_instrument(multi_track, desired_instrument)\n",
    "\n",
    "\tif track:\n",
    "\t\tplot_track(track, desired_instrument, True, 4)\n",
    "\telse:\n",
    "\t\tprint('No track found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa660f",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2851e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\tbasic_model = MusicRNN(\n",
    "\t\thidden_size=128,\n",
    "\t\tnum_pitches=129,  # 0-128 notes (including silence at 0)\n",
    "\t\tnum_layers=2,\n",
    "\t\tdropout=0.1\n",
    "\t).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52165e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\tbatched_model = MusicRNN_Batched(\n",
    "\t\thidden_size=128,\n",
    "\t\tnum_pitches=129,  # 0-128 notes (including silence at 0)\n",
    "\t\tnum_layers=2,\n",
    "\t\tdropout=0.1\n",
    "\t)\n",
    "\n",
    "\tbatched_model = batched_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e35584c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "\tlstm_model = MusicLSTM(\n",
    "\t\thidden_size=256,\n",
    "\t\tnum_pitches=num_pitches+1,  # 0-128 notes (including silence at 0)\n",
    "\t\tnum_layers=2,\n",
    "\t\tdropout=0.1\n",
    "\t).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f8513",
   "metadata": {},
   "source": [
    "## Testing (toy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13eca953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some simple test sequences\n",
    "test_seq_1 = torch.Tensor([\n",
    "\t[1, 10],\n",
    "\t[2, 10],\n",
    " \t[3, 10],\n",
    "  \t[4, 10],\n",
    "   \t[5, 10],\n",
    "    [4, 10],\n",
    "    [3, 10],\n",
    "    [2, 10],\n",
    "    [1, 10],\n",
    "\t[2, 10],\n",
    " \t[3, 10],\n",
    "  \t[4, 10],\n",
    "   \t[5, 10],\n",
    "    [4, 10],\n",
    "    [3, 10],\n",
    "    [2, 10],\n",
    "    [1, 10],\n",
    "]).float()\n",
    "\n",
    "test_seq_2 = torch.Tensor([\n",
    "\t[1, 10],\n",
    "\t[2, 10],\n",
    " \t[3, 10],\n",
    "  \t[4, 10],\n",
    "   \t[5, 10]\n",
    "]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aeedd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Overfit on 1 sequence\n",
    "if False:\n",
    "\tmodel = lstm_model\n",
    "\n",
    "\tseq = test_seq_1\n",
    "\tstart_notes = seq[0]\n",
    "\tmax_len = 100\n",
    "\n",
    "\tif True:\n",
    "\t\ttrain_batched(model, [seq], num_epochs=5000, lr=0.0001)\n",
    "\n",
    "\t\t# Test sampling a sequence\n",
    "\t\tpiano_state_samples = sample_notes(model, start_notes, max_len)\n",
    "\n",
    "\t\tplot_piano_states(seq, None, 'Real Sequence')\n",
    "\t\tplot_piano_states(piano_state_samples.piano_states, None, 'Generated Sequence')\n",
    "\t\tplot_note_sample_probs(piano_state_samples.note_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa784db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Packed Model\n",
    "seqs = [test_seq_1, test_seq_2]\n",
    "start_notes = seqs[0][0]\n",
    "max_len = 100\n",
    "\n",
    "if False:\n",
    "\ttrain_batched(\n",
    "     \tbatched_model,\n",
    "      \tseqs,\n",
    "\t\tbatch_size=2,\n",
    "       \tnum_epochs=1000,\n",
    "        lr=0.0001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f186ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Test sampling a sequence\n",
    "\tpiano_state_samples = sample_notes(\n",
    "     \tbatched_model,\n",
    "      \tstart_notes,\n",
    "       \tmax_len,\n",
    "\t\ttemperature=0.3\n",
    "    )\n",
    "\n",
    "\t# plot_piano_states(seq, None, 'Real Sequence')\n",
    "\tplot_piano_states(piano_state_samples.piano_states, None, 'Generated Sequence')\n",
    "\tplot_note_sample_probs(piano_state_samples.note_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e084ebe",
   "metadata": {},
   "source": [
    "## Testing (Real Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "215e47f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21425 total files\n",
      "Got 20 total sequences for instrument \"Bass\"\n"
     ]
    }
   ],
   "source": [
    "dataset = InstrumentDataset(\n",
    "\tinstrument='Bass',\n",
    "\tmax_samples=20,\n",
    ")\n",
    "trainloader = get_dataloader(dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89dc53f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicLSTM(\n",
      "  (rnn): LSTM(2, 256, num_layers=2, bias=False, batch_first=True, dropout=0.1)\n",
      "  (note_head): Linear(in_features=256, out_features=130, bias=True)\n",
      "  (duration_head): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "# Parameters: 822147\n"
     ]
    }
   ],
   "source": [
    "# Choose your model\n",
    "model = lstm_model\n",
    "print(model)\n",
    "print(f\"# Parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "880d3c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on data set with n = 4\n",
      "Epoch 0/200, Loss = 16730545.6250\n",
      "Epoch 1/200, Loss = 16482781.8125\n",
      "Epoch 2/200, Loss = 16391457.8125\n",
      "Epoch 3/200, Loss = 16340692.8125\n",
      "Epoch 4/200, Loss = 16307954.0000\n",
      "Epoch 5/200, Loss = 16282926.5625\n",
      "Epoch 6/200, Loss = 16268250.8125\n",
      "Epoch 7/200, Loss = 16263685.1250\n",
      "Epoch 8/200, Loss = 16263693.0625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m \t\u001b[43mtrain_batched\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m\t\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/ML/MusicGen/src/models/train.py:210\u001b[0m, in \u001b[0;36mtrain_batched\u001b[0;34m(model, dataloader, num_epochs, lr)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (packed_batch, batch_size) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    208\u001b[0m     packed_batch \u001b[38;5;241m=\u001b[39m packed_batch\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> 210\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_fully_packed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpacked_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnote_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration_criterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    220\u001b[0m epoch_losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n",
      "File \u001b[0;32m~/Desktop/projects/ML/MusicGen/src/models/train.py:175\u001b[0m, in \u001b[0;36mtrain_step_fully_packed\u001b[0;34m(model, packed_input_batch, batch_size, note_criterion, duration_criterion, optimizer, clip_grads, max_norm)\u001b[0m\n\u001b[1;32m    172\u001b[0m duration_loss \u001b[38;5;241m=\u001b[39m duration_criterion(pred_durations, target_durations)\n\u001b[1;32m    174\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m note_loss \u001b[38;5;241m+\u001b[39m duration_loss\n\u001b[0;32m--> 175\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip_grads:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# WHY? RNN/LSTM prone to exploding gradients, Allows higher learning rates, More stable training\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39mmax_norm)\n",
      "File \u001b[0;32m~/Desktop/projects/ML/venv/lib/python3.9/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/ML/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/ML/venv/lib/python3.9/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if True:\n",
    "\ttrain_batched(\n",
    "\t\tmodel,\n",
    "\t\ttrainloader,\n",
    "\t\tnum_epochs=200,\n",
    "\t\tlr=0.001\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abe5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = dataset[0]\n",
    "if True:\n",
    "\tpredictions = sample_notes(\n",
    "\t\tmodel,\n",
    "\t\tstart_event=torch.Tensor(seq[0]),\n",
    "\t\tlength=beats_per_bar*1,\n",
    "\t\ttemperature=0.3\n",
    "\t)\n",
    "\n",
    "\tplot_note_sample_probs(predictions.note_samples)\n",
    "\toutput_piannoroll_to_midi(\n",
    "     \tpredictions.piano_states,\n",
    "\t\tinstrument='Guitar',\n",
    "     \tname='generated_guitar2'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
